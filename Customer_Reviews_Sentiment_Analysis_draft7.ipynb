{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KemasRaihan/Sentiment-Analysis-Of-Social-Media-Posts-Of-Phones-Using-Hybrid-Neural-Networks/blob/main/Customer_Reviews_Sentiment_Analysis_draft7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8IaW8SY0tfW"
      },
      "source": [
        "# Import Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjUi1t76xao0"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W8T5x7HVuJiN"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "import matplotlib.pyplot as plt\n",
        "# % matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q5nD7z88QV3a"
      },
      "outputs": [],
      "source": [
        "#For Qualitative Analysis\n",
        "from itertools import groupby\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lZpr9W-QcUO",
        "outputId": "6e03cb91-bba9-4966-ca7c-493eb021aa51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# For Text Preprocessing\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import re\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RYbVkQF3pQjF"
      },
      "outputs": [],
      "source": [
        "# For dataset partitioning\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_curve,auc\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uBVHMlQFQmoo"
      },
      "outputs": [],
      "source": [
        "# For building the neural networks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "from keras import models, layers\n",
        "#from keras.layers import Embedding, Flatten, Conv1D, MaxPooling1D, GlobalMaxPooling1D, SpatialDropout1D, LSTM, Bidirectional, Dense, Dropout\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import plot_model\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer"
      ],
      "metadata": {
        "id": "bbr5I1sMZ0Hw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kZLQAogsQtWp"
      },
      "outputs": [],
      "source": [
        "# For testing and evaluation\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "#from keras.wrappers import KerasClassifier\n",
        "import seaborn as sns\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY2tXOwOyC1a"
      },
      "source": [
        "## Other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ECQo8Phx-Mr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507793d4-c745-467d-c3a8-1bd53d8bad79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Btbbnf9sioAh"
      },
      "outputs": [],
      "source": [
        "# Directory path\n",
        "dirpath = '/content/drive/MyDrive/Sheffield_Hallam_University/Final Year/Development Project/development_project_shared/Source_Code/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "i0sJDXBS7wPt"
      },
      "outputs": [],
      "source": [
        "# Define classes\n",
        "# There are 3 classes: negative, neutral and postive\n",
        "sentiment_classes = [0,1,2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPOtM9imlXX4"
      },
      "source": [
        "# Load Files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Models**"
      ],
      "metadata": {
        "id": "tezo8_rAZwqF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zNhvPkda9Mx"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "def load_model_from_path(name):\n",
        "  # Define file name\n",
        "  file = name + '_sentiment_predictor.h5'\n",
        "\n",
        "  # Define path to file\n",
        "  path = dirpath + '/models/' + file\n",
        "\n",
        "  # Load model from path\n",
        "  model = tf.keras.models.load_model(\n",
        "    path, custom_objects=None, compile=True\n",
        "    )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JuJq4uGbcQ9"
      },
      "outputs": [],
      "source": [
        "#CNN_model = load_model_from_path('CNN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zq_281Mbe9w"
      },
      "outputs": [],
      "source": [
        "#LSTM_model = load_model_from_path('LSTM')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BiLSTM_model = load_model_from_path('BiLSTM')"
      ],
      "metadata": {
        "id": "jWbeTKhZWqmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfzVicYwbjVZ"
      },
      "outputs": [],
      "source": [
        "#HybridCL_model = load_model_from_path('CNN_LSTM')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HybridCB_model = load_model_from_path('CNN_BiLSTM')"
      ],
      "metadata": {
        "id": "N_oEwsQBWTO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Times**"
      ],
      "metadata": {
        "id": "5C-xS_oDZy7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#open and read the file after the appending:\n",
        "# f = open(\"training_times.txt\", \"r\")\n",
        "# print(f.read())"
      ],
      "metadata": {
        "id": "aay1kBKnZ1dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a4JfGJuv7Kh"
      },
      "source": [
        "# Amazon Customer Reviews Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ia1Kaj78lbA"
      },
      "source": [
        "## Import Raw Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHyDRJuZFZp-"
      },
      "source": [
        "**Raw Dataset**\n",
        "\n",
        "PromptCloud extracted 400 thousand reviews of unlocked mobile phones sold on Amazon.com to find out insights with respect to reviews, ratings, price and their relationships (https://www.kaggle.com/datasets/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQFro86Z8i_m"
      },
      "outputs": [],
      "source": [
        "# Import Raw Dataset\n",
        "filepath = dirpath + 'Amazon_Unlocked_Mobile.csv'\n",
        "\n",
        "# Control number of rows to read from csv file\n",
        "nrows = 4000\n",
        "\n",
        "df = pd.read_csv(filepath,nrows=nrows, encoding = 'latin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvbXKNfk8ns-"
      },
      "source": [
        "## Examination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMK3kYSzXz2x"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-psPCwUWcFwP"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU8o7HX6p9zU"
      },
      "source": [
        "## Drop NA Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO9ERtt5xyrN"
      },
      "outputs": [],
      "source": [
        "# Fine any null rows\n",
        "df.isnull().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68yCHng5xyC2"
      },
      "outputs": [],
      "source": [
        "# Drop all null rows from dataframe\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QnkoiGH7AnW"
      },
      "source": [
        "## View Frequency Of Ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVmntDsG4Gzz"
      },
      "outputs": [],
      "source": [
        "# Convert rating to list for further processing\n",
        "ratings = df['Rating'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTuiTR5B5VHg"
      },
      "outputs": [],
      "source": [
        "ratings[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib-IJJ4tZS9s"
      },
      "outputs": [],
      "source": [
        "frequency = []\n",
        "\n",
        "# For each rating\n",
        "for i in range(1,6):\n",
        "  # Count each rating that occurs in ratings and append to frequency array\n",
        "  frequency.append(ratings.count(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB7eogaR6I7E"
      },
      "outputs": [],
      "source": [
        "print(frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA1H2QEd6V_i"
      },
      "outputs": [],
      "source": [
        "labels = [1,2,3,4,5]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.set_title('Percentage Of Reviews By Ratings')\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "pps = ax.pie(frequency, labels=labels, autopct='%1.1f%%')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LePM89N5czyc"
      },
      "source": [
        "## Reduce Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyXxeAwaRf9p"
      },
      "outputs": [],
      "source": [
        "sentiments = []\n",
        "ratings = df['Rating']\n",
        "\n",
        "for rating in ratings:\n",
        "  # if rating is 1 or 2 (negative) append 0 to sentiments array\n",
        "  if rating < 3:\n",
        "    sentiments.append(0)\n",
        "  # if rating is 3 (neutral), append 1 to sentiments array\n",
        "  elif rating == 3:\n",
        "    sentiments.append(1)\n",
        "  # if rating is 4 or 5 (positive), append 2 to sentiments array\n",
        "  else:\n",
        "    sentiments.append(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR8Md2bv5B-d"
      },
      "outputs": [],
      "source": [
        "frequency = []\n",
        "\n",
        "for i in range(3):\n",
        "  frequency.append(sentiments.count(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx9Sr6HubBnE"
      },
      "outputs": [],
      "source": [
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "colors = ['red', 'orange', 'green']\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.set_title('Percentage Of Reviews By Ratings')\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "pps = ax.pie(frequency, labels=labels, autopct='%1.1f%%', colors=colors)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePSUUaJcmd3g"
      },
      "outputs": [],
      "source": [
        "# Append new column to original dataframe\n",
        "df = df.assign(Sentiment=sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTzM3Mv9xU_D"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tdlj4m8xqEP"
      },
      "outputs": [],
      "source": [
        "df.to_csv(dirpath + 'Amazon_Unlocked_Mobile_Updated.csv', sep=',', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzMYQjuQyz46"
      },
      "source": [
        "# Preprocessing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1lkpsvuQ9ws"
      },
      "outputs": [],
      "source": [
        "reviews = df['Reviews']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsGEyS933Vyd"
      },
      "outputs": [],
      "source": [
        "# Reviews before preprocessing\n",
        "reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4LlbJCcD-fw"
      },
      "outputs": [],
      "source": [
        "len(reviews[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords 'n punctuation\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Update Stopwords To Exclude the word 'phone'\n",
        "STOPWORDS.update([\"phone\", \"Phone\"])"
      ],
      "metadata": {
        "id": "fSx0KI_xCI5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqtLGRIkSp7H"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(review):\n",
        "\n",
        "    # Convert to lowercase\n",
        "    review = review.lower()\n",
        "\n",
        "     # Remove words with non-ASCII characters\n",
        "    review = re.sub(r'[^\\x00-\\x7F]+',' ', review)\n",
        "\n",
        "    # Remove stop words\n",
        "    words = review.split()\n",
        "    words = [word for word in words if word not in STOPWORDS]\n",
        "\n",
        "    review = \" \".join(words)\n",
        "\n",
        "    return review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRZTSASJyA0J"
      },
      "outputs": [],
      "source": [
        "reviews = reviews.apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPdMeYoXWLvW"
      },
      "outputs": [],
      "source": [
        "# Reviews after preprocessing\n",
        "reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyY-wcDRD87L"
      },
      "outputs": [],
      "source": [
        "len(reviews[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xmfiCF3qM3t"
      },
      "source": [
        "# Dataset Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp8JtCxA686C"
      },
      "source": [
        "## Quantitative Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYIf6Xez6_my"
      },
      "outputs": [],
      "source": [
        "# Group by sentiment\n",
        "df_sentiment = df.groupby('Sentiment')\n",
        "\n",
        "# Define sentiment labekls\n",
        "\n",
        "# initialise array to store each boxplot data\n",
        "data = []\n",
        "\n",
        "for sentiment in sentiment_classes:\n",
        "  # Group dataframe by the rating\n",
        "  dfr = df_sentiment.get_group(sentiment)\n",
        "\n",
        "  prices = np.array(dfr['Price'])\n",
        "\n",
        "  data.append(prices)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize =(9, 7))\n",
        "\n",
        "# Creating axes instance\n",
        "ax = fig.add_axes([1, 1, 1, 1])\n",
        "\n",
        "ax.set_title('Boxplot of Sentiment by Price')\n",
        "ax.set_ylabel('Price')\n",
        "ax.set_xlabel('Sentiment')\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "# Creating plot\n",
        "bp = ax.boxplot(data,\n",
        "           patch_artist = True,\n",
        "           boxprops = dict(facecolor = \"lightblue\"),\n",
        "           showfliers=False)\n",
        "# show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HljKhcYKU4-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYllvghf66br"
      },
      "source": [
        "## Qualitative Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_-Kw-4R0s6Y"
      },
      "outputs": [],
      "source": [
        "# Creating an object using groupby\n",
        "df_sentiments = df.groupby('Sentiment')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nouns(text):\n",
        "    blob = TextBlob(text)\n",
        "    return [word for (word,tag) in blob.tags if tag == \"NN\"]"
      ],
      "metadata": {
        "id": "Ai6_GemlgOV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = df['Reviews']"
      ],
      "metadata": {
        "id": "J6gawprhjo0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviews.head())"
      ],
      "metadata": {
        "id": "IM4Jzrr8jrx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVXS7ix-rsn6"
      },
      "outputs": [],
      "source": [
        "def generate_wordcloud(sentiment, ax, title, cm):\n",
        "\n",
        "    # Group dataframe by the sentiment\n",
        "    df = df_sentiments.get_group(sentiment)\n",
        "\n",
        "    # Circle mask\n",
        "    x, y = np.ogrid[:300, :300]\n",
        "    mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
        "    mask = 255 * mask.astype(int)\n",
        "\n",
        "    # Creating the text variable\n",
        "    text = \" \".join(review for review in df['Reviews'])\n",
        "\n",
        "    # Creating word cloud with text as argument in .generate() method\n",
        "    wordcloud = WordCloud(collocations=False, background_color='white', colormap=cm, mask=mask, stopwords=STOPWORDS).generate(text)\n",
        "\n",
        "    # Display the generated Word Cloud\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title)\n",
        "    # ax.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EMjjc0C6Sua"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,3, figsize=(18,6))\n",
        "fig.suptitle('Word Cloud For Each Sentiment', fontsize=20)\n",
        "\n",
        "# Generate wordcloud of negative reviews\n",
        "generate_wordcloud(0, axes[0],  'negative', 'Reds')\n",
        "\n",
        "# Generate wordcloud of neutral reviews\n",
        "generate_wordcloud(1, axes[1],  'neutral', 'Purples')\n",
        "\n",
        "# Generate wordcloud of positive reviews\n",
        "generate_wordcloud(2, axes[2],  'positive', 'Greens')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEb8qaOdyBb3"
      },
      "source": [
        "# Define Training, Validation and Testing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsKiVm2ggZmh"
      },
      "source": [
        "## Preparing Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhxqf7Pggfy0"
      },
      "outputs": [],
      "source": [
        "# Padding all reviews to fixed length 100\n",
        "maxlen = 100\n",
        "\n",
        "word_tokenizer = Tokenizer()\n",
        "\n",
        "word_tokenizer.fit_on_texts(reviews)\n",
        "\n",
        "vocab_size = len(word_tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V10NIV0ggRO"
      },
      "outputs": [],
      "source": [
        "embeddings_dictionary = dict()\n",
        "glove_file = open(dirpath + 'a2_glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0x3Ul8TgojF"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty matrix with zeros, where the number of rows is determined by the vocabulary size (vocab_size),\n",
        "# and the number of columns is set to 100, matching the dimensionality of the GloVe word embeddings.\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "# Iterate through each word in the tokenizer's word index\n",
        "for word, i in word_tokenizer.word_index.items():\n",
        "  # Retrieve the GloVe word embedding vector for the current word\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    # Check if the word has a corresponding embedding in the GloVe model\n",
        "    if embedding_vector is not None:\n",
        "      # If an embedding exists, update the corresponding row in the embedding matrix\n",
        "        # with the GloVe word embedding vector for the current word.\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define X and Y"
      ],
      "metadata": {
        "id": "h5wfS__L9dgs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57UUxionUcd0"
      },
      "outputs": [],
      "source": [
        "one_hot_encoded_Y = pd.get_dummies(sentiments).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG1RyGt7uuPM"
      },
      "outputs": [],
      "source": [
        "one_hot_encoded_Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29S2xKm8usLS"
      },
      "outputs": [],
      "source": [
        "reviews.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_0UpA_7YfDO"
      },
      "outputs": [],
      "source": [
        "# Firstly split them into the training dataset and the rest as remaining dataset for validation and testing\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(reviews, one_hot_encoded_Y, train_size=0.8)\n",
        "\n",
        "# Split the remaining dataset for validation and testing\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, train_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdeVDKl1oM14"
      },
      "outputs": [],
      "source": [
        "print(X_train[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXXmxxOqzuRF"
      },
      "outputs": [],
      "source": [
        "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
        "X_valid = word_tokenizer.texts_to_sequences(X_valid)\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_valid = pad_sequences(X_valid, padding='post', maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wy4iM0uD1Yo"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT_cIW-GEHdX"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7y7lj2cPG2c"
      },
      "outputs": [],
      "source": [
        "X_valid.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uj8or0FxGld"
      },
      "source": [
        "# Build and Train The Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DRcK1LOT5yY"
      },
      "outputs": [],
      "source": [
        "# Number of Epochs for each model to train\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_times = {}"
      ],
      "metadata": {
        "id": "sqQlXy_-YG5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to add value labels\n",
        "def addlabels(x,y):\n",
        "    for i in range(len(x)):\n",
        "        plt.text(i,y[i],y[i])"
      ],
      "metadata": {
        "id": "RSjpPQM2XzCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1nWWFq0-P_t"
      },
      "outputs": [],
      "source": [
        "def display_graphs(history):\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(17, 5))\n",
        "  ax[0].plot(history.history['acc'])\n",
        "  ax[0].plot(history.history['val_acc'])\n",
        "\n",
        "  ax[0].set_title('Model Accuracy')\n",
        "  ax[0].set_ylabel('Accuracy')\n",
        "  ax[0].set_xlabel('Epochs')\n",
        "  ax[0].legend(['train','test'], loc='upper left')\n",
        "\n",
        "  ax[1].plot(history.history['loss'])\n",
        "  ax[1].plot(history.history['val_loss'])\n",
        "\n",
        "  ax[1].set_title('Model Loss')\n",
        "  ax[1].set_xlabel('Epochs')\n",
        "  ax[1].legend(['train','test'], loc='upper left')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsZQxdY6gKzx"
      },
      "source": [
        "## Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGspzk61gOD-"
      },
      "outputs": [],
      "source": [
        "# Building convolutional neural network model\n",
        "def create_CNN_model(vocab_size, max_len, embedding_matrix):\n",
        "  model = models.Sequential()\n",
        "  # Input - Layer\n",
        "  model.add(layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False))\n",
        "  # Hidden - Layers\n",
        "  model.add(layers.Conv1D(128, kernel_size=5, activation='relu'))\n",
        "  model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "  model.add(layers.Flatten())\n",
        "  # Output- Layer\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  # Define optimiser for CNN model\n",
        "  optimiser = keras.optimizers.RMSprop(learning_rate=2e-4)\n",
        "\n",
        "  # Compile model\n",
        "  model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['acc'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_model = create_CNN_model(vocab_size, max_len, embedding_matrix)"
      ],
      "metadata": {
        "id": "-2RVr50ZUgjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4KdtNudyosJ"
      },
      "outputs": [],
      "source": [
        "plot_model(\n",
        "  CNN_model,\n",
        "  show_shapes = True,\n",
        "  show_dtype = True,\n",
        "  show_layer_activations = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeJOrhsCg6dm"
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\n",
        "CNN_history = CNN_model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_valid, y_valid))\n",
        "CNN_ts = time.time() - t0\n",
        "training_times['CNN'] = CNN_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AZFQWedkZD5"
      },
      "outputs": [],
      "source": [
        "display_graphs(CNN_model.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VgjF6FaJg6g"
      },
      "source": [
        "## Long-Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1AmbU7dIqA3"
      },
      "outputs": [],
      "source": [
        "# Build an LSTM model\n",
        "def create_LSTM_model(vocab_size, max_len, embedding_matrix):\n",
        "  model = models.Sequential()\n",
        "\n",
        "  # Input - Layer\n",
        "  model.add(layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False))\n",
        "  # Hidden - Layers\n",
        "  model.add(layers.LSTM(128, return_sequences=True))\n",
        "  model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "  model.add(layers.Flatten())\n",
        "  # Output- Layer\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  # Define optimiser for LSTM model\n",
        "  optimiser = keras.optimizers.RMSprop(learning_rate=2e-4)\n",
        "\n",
        "  # Compile model\n",
        "  model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['acc'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_model = create_LSTM_model(vocab_size, max_len, embedding_matrix)"
      ],
      "metadata": {
        "id": "OX5oWpa1UVXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RANkg1RvAPXU"
      },
      "outputs": [],
      "source": [
        "plot_model(\n",
        "    LSTM_model,\n",
        "    show_shapes = True,\n",
        "    show_dtype = True,\n",
        "    show_layer_activations = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPonXcP3V0K8"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "t0 = time.time()\n",
        "LSTM_history = LSTM_model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_valid, y_valid))\n",
        "LSTM_ts = time.time()-t0\n",
        "training_times['LSTM'] = LSTM_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCFJKFP7kcza"
      },
      "outputs": [],
      "source": [
        "display_graphs(LSTM_model.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgiHhEPBYnu0"
      },
      "source": [
        "## Bidirectional Long-Short Term Memory (Bi-LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgHMgzoKYnvL"
      },
      "outputs": [],
      "source": [
        "# Build an LSTM model\n",
        "def create_BiLSTM_model(vocab_size, max_len, embedding_matrix):\n",
        "  model = models.Sequential()\n",
        "\n",
        "  # Input - Layer\n",
        "  model.add(layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False))\n",
        "  # Hidden - Layers\n",
        "  model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
        "  model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "  model.add(layers.Flatten())\n",
        "  # Output- Layer\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  # Define optimiser for LSTM model\n",
        "  optimiser = keras.optimizers.RMSprop(learning_rate=2e-4)\n",
        "\n",
        "  # Compile model\n",
        "  model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['acc'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BiLSTM_model = create_BiLSTM_Model(vocab_size, max_len, embedding_matrix)"
      ],
      "metadata": {
        "id": "MQKE9cU1IGZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvRTCNPTYnvM"
      },
      "outputs": [],
      "source": [
        "plot_model(\n",
        "    BiLSTM_model,\n",
        "    show_shapes = True,\n",
        "    show_dtype = True,\n",
        "    show_layer_activations = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giPGZvOlYnvM"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "t0 = time.time()\n",
        "BiLSTM_history = BiLSTM_model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_valid, y_valid))\n",
        "BiLSTM_ts = time.time()-t0\n",
        "training_times['BiLSTM'] = BiLSTM_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpjhmrekYnvN"
      },
      "outputs": [],
      "source": [
        "display_graphs(BiLSTM_model.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkZe3gLfh_PN"
      },
      "source": [
        "## CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwDwwx4LiBhO"
      },
      "outputs": [],
      "source": [
        "# Build an CNN + LSTM model\n",
        "def create_HybridCL_model(vocab_size, max_len, embedding_matrix):\n",
        "  model = models.Sequential()\n",
        "\n",
        "  # Input - Layer\n",
        "  model.add(layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False))\n",
        "  # Hidden - Layers\n",
        "  # CNN layers\n",
        "  model.add(layers.Conv1D(128, kernel_size=5, activation='relu'))\n",
        "  model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "  # LSTM layers\n",
        "  model.add(layers.LSTM(128, return_sequences=True))\n",
        "  model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "  model.add(layers.Flatten())\n",
        "  # Output- Layer\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  # Define optimiser for CNN + Dense model\n",
        "  optimiser = keras.optimizers.RMSprop(learning_rate=2e-4)\n",
        "\n",
        "  # Compile model\n",
        "  model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['acc'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HybridCL_model = create_HybridCL_model(vocab_size, max_len, embedding_matrix)"
      ],
      "metadata": {
        "id": "NVkJhLHgH1AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHwp-UpjHydV"
      },
      "outputs": [],
      "source": [
        "plot_model(\n",
        "    HybridCL_model,\n",
        "    show_shapes = True,\n",
        "    show_dtype = True,\n",
        "    show_layer_activations = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyq-zjlGIyrY"
      },
      "outputs": [],
      "source": [
        "# Train the CNN + LSTM model\n",
        "t0 = time.time()\n",
        "HybridCL_history = HybridCL_model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_valid, y_valid))\n",
        "HybridCL_ts = time.time() - t0\n",
        "training_times['CNN + LSTM'] = HybridCL_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptn2Y8UpIHhs"
      },
      "outputs": [],
      "source": [
        "display_graphs(HybridCL_model.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_sevFPn-Kn8"
      },
      "source": [
        "## CNN + Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2kdATuf-MvX"
      },
      "outputs": [],
      "source": [
        "# Build an CNN + LSTM model\n",
        "def create_HybridCB_model(vocab_size, max_len, embedding_matrix):\n",
        "  model = models.Sequential()\n",
        "\n",
        "  # Input - Layer\n",
        "  model.add(layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False))\n",
        "  # Hidden - Layers\n",
        "  # CNN layers\n",
        "  model.add(layers.Conv1D(128, kernel_size=5, activation='relu'))\n",
        "  model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "  # LSTM layers\n",
        "  model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
        "  model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "  model.add(layers.Flatten())\n",
        "  # Output- Layer\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  # Define optimiser for CNN + Dense model\n",
        "  optimiser = keras.optimizers.RMSprop(learning_rate=2e-4)\n",
        "\n",
        "  # Compile model\n",
        "  model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['acc'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HybridCB_model = create_HybridCB_model(vocab_size, max_len, embedding_matrix)"
      ],
      "metadata": {
        "id": "jwT3Cjq4Hk-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Auiqu-lA-NsB"
      },
      "outputs": [],
      "source": [
        "plot_model(\n",
        "    HybridCB_model,\n",
        "    show_shapes = True,\n",
        "    show_dtype = True,\n",
        "    show_layer_activations = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CNN + LSTM model\n",
        "t0 = time.time()\n",
        "HybridCB_history = HybridCB_model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_valid, y_valid))\n",
        "HybridCB_ts = time.time() - t0\n",
        "training_times['CNN + BiLSTM'] = HybridCB_ts"
      ],
      "metadata": {
        "id": "GwySRhZgP1t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_RHwCGg76QE"
      },
      "outputs": [],
      "source": [
        "display_graphs(HybridCB_model.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOTfnRAFDlNo"
      },
      "source": [
        "# Testing and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [CNN_model, LSTM_model, BiLSTM_model, HybridCL_model, HybridCB_model]\n",
        "names = ['CNN', 'LSTM', 'BiLSTM', 'CNN + LSTM', 'CNN + BiLSTM']"
      ],
      "metadata": {
        "id": "QTnaMH7uUfjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdgy-IOLEH-u"
      },
      "outputs": [],
      "source": [
        "def test_model(model, review):\n",
        "  pred = model.predict(review)\n",
        "  pred = pred.flatten()\n",
        "  return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vOaIIfDEUII"
      },
      "outputs": [],
      "source": [
        "def preprocess_review(review):\n",
        "  review = preprocess_text(review)\n",
        "  review = word_tokenizer.texts_to_sequences([review])\n",
        "  review = pad_sequences(review, padding='post', maxlen=maxlen)\n",
        "  return review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5X-r5jhn1z9"
      },
      "outputs": [],
      "source": [
        "def draw_piecharts(review):\n",
        "\n",
        "  print(\"Review: \", review)\n",
        "\n",
        "  review = preprocess_review(review)\n",
        "\n",
        "  labels = ['Negative', 'Neutral', 'Positive']\n",
        "  colors = ['red', 'orange', 'green']\n",
        "\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(18, 10))\n",
        "  plt.title('Neural Network Predictions', fontsize=14)\n",
        "  #2 rows 2 columns\n",
        "  i = 0\n",
        "  for i in range(len(models)):\n",
        "    pred = test_model(models[i], review)\n",
        "    ax[i].pie(pred,colors=colors, labels=labels, autopct='%1.1f%%')\n",
        "    ax[i].set_title(names[i])\n",
        "    i=i+1\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to add value labels\n",
        "def addlabels(x,y):\n",
        "    for i in range(len(x)):\n",
        "        plt.text(i,y[i],y[i])"
      ],
      "metadata": {
        "id": "hXrTriDKkf6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nsUvbvDnfBo"
      },
      "source": [
        "## User Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQTsO1IgXJpR"
      },
      "source": [
        "The expected output should be one vector that represent the probability distribution of each sentiment prediction: negative, neutral and positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srvNV0KOd8HO"
      },
      "outputs": [],
      "source": [
        "#input_review = input(\"Enter review: \")\n",
        "input_review = 'I like this phone'\n",
        "draw_piecharts(input_review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ie9NEKFnh3O"
      },
      "source": [
        "## Random Sample Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hukFkjvFHx4q"
      },
      "outputs": [],
      "source": [
        "X_test = X_test.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RnhZ5xQIEwd"
      },
      "outputs": [],
      "source": [
        "X_test[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAy_BoMvVsp8"
      },
      "outputs": [],
      "source": [
        "random_num = random.randint(0, len(X_test)-1)\n",
        "\n",
        "random_review = X_test[random_num]\n",
        "actual = y_test[random_num]\n",
        "print(random_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7B5LLE7nysX"
      },
      "outputs": [],
      "source": [
        "print('Actual: ', actual)\n",
        "draw_piecharts(random_review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwufbb3Pia2_"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Scores and Accuracies"
      ],
      "metadata": {
        "id": "CCE_MaxotdrF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUlEQOC3LTsm"
      },
      "outputs": [],
      "source": [
        "X_test_processed = word_tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_test_processed = pad_sequences(X_test_processed, padding='post', maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ7ZwVVFZALQ"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(model):\n",
        "  # Predictions on the Test Set\n",
        "  score = model.evaluate(X_test_processed, y_test, verbose=1)\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzwJ1WXXLsUg"
      },
      "outputs": [],
      "source": [
        "def plot_scores_bar():\n",
        "  accuracy_scores = []\n",
        "  test_scores = []\n",
        "  test_accuracies = []\n",
        "\n",
        "  # For each model calculate the accuracy scores\n",
        "  for model in models:\n",
        "    accuracy_scores.append(calc_accuracy(model))\n",
        "\n",
        "  # Extract each test scores and test accuracy of each model\n",
        "  for score in accuracy_scores:\n",
        "    test_scores.append(score[0])\n",
        "    test_accuracies.append(score[1])\n",
        "\n",
        "  X_axis = np.arange(len(names))\n",
        "\n",
        "  plt.bar(X_axis - 0.2, test_scores, 0.4, label = 'Test Scores')\n",
        "  plt.bar(X_axis + 0.2, test_accuracies, 0.4, label = 'Test Accuracies')\n",
        "\n",
        "  plt.xticks(X_axis, names, rotation=45, ha='right')\n",
        "  plt.xlabel(\"Model\")\n",
        "  plt.title(\"Scores\")\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRCf6ZHENfoT"
      },
      "outputs": [],
      "source": [
        "plot_scores_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLy84zIaWuVS"
      },
      "source": [
        "### Accuracy, precision, recall and f1 scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P3XcZW5Vmb5"
      },
      "outputs": [],
      "source": [
        "# Convert multidimensional y array to a one-dimensional array\n",
        "def convert_y_to_1D(y):\n",
        "  return np.argmax(y, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FO4aOmAWtgf"
      },
      "outputs": [],
      "source": [
        "#Calculates accuracy, precision, recall, f1 and specificity from predicted and true results\n",
        "def calc_scores(model, X_fold, y_true):\n",
        "  # Calculate predicted values\n",
        "  y_pred = model.predict(X_fold)\n",
        "\n",
        "  # Convert y_true and y_pred to one-dimensional arrays\n",
        "  y_true, y_pred = convert_y_to_1D(y_true), convert_y_to_1D(y_pred)\n",
        "\n",
        "  # Calculate accuracy score\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "  # Calculate precision score\n",
        "  precision = precision_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "  # Calculate recall score\n",
        "  recall = recall_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "  # Calculate f1 score\n",
        "  f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "  return [accuracy, precision, recall, f1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHV0GilNTPoR"
      },
      "outputs": [],
      "source": [
        "#Calculates accuracy, precision, recall, f1 and specificity from predicted and true results\n",
        "def calc_scores(model, X_fold, y_true):\n",
        "  # Calculate predicted values\n",
        "  y_pred = model.predict(X_fold)\n",
        "\n",
        "  # Convert y_true and y_pred to one-dimensional arrays\n",
        "  y_true, y_pred = convert_y_to_1D(y_true), convert_y_to_1D(y_pred)\n",
        "\n",
        "  # Calculate accuracy score\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "  # Calculate precision score\n",
        "  precision = precision_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "  # Calculate recall score\n",
        "  recall = recall_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "  # Calculate f1 score\n",
        "  f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "  return [accuracy, precision, recall, f1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW7A69Puhemq"
      },
      "outputs": [],
      "source": [
        "def create_scores_dict(X_fold, y_fold):\n",
        "  scores_dict = {}\n",
        "\n",
        "  for i in range(len(models)):\n",
        "    # Calculate the scores (accuracy, precision, recall and F1) of the model\n",
        "    scores = calc_scores(models[i], X_fold, y_fold)\n",
        "\n",
        "    # Round each value to 2 d.p.\n",
        "    #scores = [ '%.4f' % score for score in scores ]\n",
        "\n",
        "    # Create sub dictionary for the scores of the model\n",
        "    model_scores = {'Accuracy' : scores[0],\n",
        "              'Precision' : scores[1],\n",
        "              'Recall' : scores[2],\n",
        "              'F1' : scores[3]}\n",
        "\n",
        "    # Assign scores to the model in the parent dictionary\n",
        "    scores_dict[names[i]] = model_scores\n",
        "\n",
        "  return scores_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEdIEs1zW_gz"
      },
      "outputs": [],
      "source": [
        "# Define the model names and their scores for the training set\n",
        "scores_train = create_scores_dict(X_train, y_train)\n",
        "\n",
        "# Define the model names and their scores for the training set\n",
        "scores_valid = create_scores_dict(X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dataframe table of scores for the training set\n",
        "scores_train_df = pd.DataFrame(scores_train).T\n",
        "scores_train_df"
      ],
      "metadata": {
        "id": "vRzkP4amIMLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dataframe table of scores for the validation set\n",
        "scores_valid_df = pd.DataFrame(scores_valid).T\n",
        "scores_valid_df"
      ],
      "metadata": {
        "id": "fscebpAQq8CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From AL and ML 1 Assignment:** Based on table it is evident that the accuracy score for KNN and SVM models for the training dataset is slightly higher than their respective accuracy score for the validation dataset. However, the LR and DT models have much higher training accuracy score compared to its validation accuracy score. The results for precision, recall, f1, and specificity show a similar pattern, with the DT model having the highest scores for its training set but SVM having the highest scores for validation."
      ],
      "metadata": {
        "id": "SK9D01q18CXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
        "\n",
        "# Define models\n",
        "models = scores_train_df.index\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "# Define colors\n",
        "colors = ['blue', 'orange']\n",
        "\n",
        "# Bar plot for Accuracy\n",
        "for i, metric in enumerate(['Accuracy', 'Precision', 'Recall', 'F1']):\n",
        "    ax = axes[i // 2, i % 2]\n",
        "    ax.bar(X_axis - 0.2, scores_train_df[f'{metric}'], width=0.4, color='blue', label='Train')\n",
        "    ax.bar(X_axis + 0.2, scores_valid_df[f'{metric}'], width=0.4, color='orange', label='Valid')\n",
        "    ax.set_title(metric)\n",
        "    ax.set_xticks(X_axis)\n",
        "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.title('Scores For The Models')\n",
        "\n",
        "# Plot legend\n",
        "plt.legend(bbox_to_anchor=(1.2, 2.1), loc='upper right', fontsize=\"12\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3da0QjL_R5d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Validation (CV) Scheme"
      ],
      "metadata": {
        "id": "CeGiMsTsy-dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way we could solve overfitting is using the Cross-validation (CV) scheme, one of the more common solutions being KFolds (Loukas 2023). Using a KFolds scheme, we wiil train and test each model k-times on different subsets of the training data and estimate a performance metric using each test data."
      ],
      "metadata": {
        "id": "fTyGb78czkON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the CNN model in a KerasClassifier\n",
        "CNN_KC = KerasClassifier(build_fn=create_CNN_model, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Wrap the LSTM model in a KerasClassifier\n",
        "LSTM_KC = KerasClassifier(build_fn=create_LSTM_model, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Wrap the BiLSTM model in a KerasClassifier\n",
        "BiLSTM_KC = KerasClassifier(build_fn=create_BiLSTM_model, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Wrap the Hybrid CNN + LSTM model in a KerasClassifier\n",
        "HybridCL_KC = KerasClassifier(build_fn=create_HybridCL_model, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Wrap the Hybrid CNN + BiLSTM model in a KerasClassifier\n",
        "HybridCB_KC = KerasClassifier(build_fn=create_HybridCB_model, epochs=10, batch_size=32, verbose=0)"
      ],
      "metadata": {
        "id": "uQ7jxeO0HP1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_cross_valid_scores(model, X_train, y_train):\n",
        "  k_folds = KFold(n_splits = 5)\n",
        "  scores = cross_val_score(model, X=X_train, y=y_train, cv=k_folds)\n",
        "  print('Cross Validation accuracy scores: %s' % scores)\n",
        "  print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
      ],
      "metadata": {
        "id": "TGqMr4m6zB3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_cross_valid_scores(CNN_KC, X_train, y_train)"
      ],
      "metadata": {
        "id": "Kvrg_10hzhOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_cross_valid_scores(LSTM_KC, X_train, y_train)"
      ],
      "metadata": {
        "id": "gdUo8lm8zpM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_cross_valid_scores(BiLSTM_KC, X_train, y_train)"
      ],
      "metadata": {
        "id": "9Zk7ggjWzynZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_cross_valid_scores(HybridCL_KC, X_train, y_train)"
      ],
      "metadata": {
        "id": "i1N5TCRzz0MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_cross_valid_scores(HybridCB_KC, X_train, y_train)"
      ],
      "metadata": {
        "id": "L-kqYCccz4qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From AL and ML 1 assignment >>**\n",
        "\n",
        "The mean cross-validation score for the DT model is the lowest among all the other machine learning models. These results suggest that the KNN and SVM models may have been slightly overfit, with SVM being the least overfit since it has the highest cross validation mean. The DT model has scored the highest accuracy for the training dataset, but it has been significantly overfit compared to the other models due to having the largest difference its accuracy scores for the training and validation dataset. Decision trees, being both non-parametric and non-linear machine learning algorithm, are known to be highly flexible and have a high potential for overfitting the training data. This is expected since overfitting is more likely with non-parametric and non-linear models (Brownlee 2019). LR is both linear and parametric but due to our dataset containing too many features (1215 to be exact). it has the second highest difference between its training and validation accuracy score."
      ],
      "metadata": {
        "id": "N0NrqEzL2J-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Time"
      ],
      "metadata": {
        "id": "Rw-kdtILR3EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe table of the dictionary of the models ad their time\n",
        "training_times_df = pd.DataFrame.from_dict(training_times, orient='index')\n",
        "\n",
        "# Rename the first column\n",
        "training_times_df.rename(columns={training_times_df.columns[0]: 'Training Time (s)'}, inplace=True)\n",
        "\n",
        "training_times_df"
      ],
      "metadata": {
        "id": "b_n8SiyPR2UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_times_bar():\n",
        "  X_axis = np.arange(len(names))\n",
        "\n",
        "  # Get the training times of each model from the dataframe and define it as y\n",
        "  y = training_times_df['Training Time (s)'].apply(lambda x: round(x, 2))\n",
        "\n",
        "  plt.bar(X_axis, y, 0.4)\n",
        "\n",
        "  plt.xticks(X_axis, names, rotation=45, ha='right')\n",
        "\n",
        "  # calling the function to add value labels\n",
        "  addlabels(X_axis, y)\n",
        "\n",
        "  plt.xlabel(\"Model\")\n",
        "  plt.ylabel(\"Training Time (s)\")\n",
        "  plt.title(\"Training Time For The Models\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Oz1UzHxPZlYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_times_bar()"
      ],
      "metadata": {
        "id": "vAbEookdcd5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p8djyeDC_VO"
      },
      "source": [
        "# Save and Store"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Models**"
      ],
      "metadata": {
        "id": "8bUE-ZR-Y_k3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1eE6AAeb9Vz"
      },
      "outputs": [],
      "source": [
        "def save_model(model, name):\n",
        "  file = name + '_sentiment_predictor.h5'\n",
        "  path = dirpath + '/models/' + file\n",
        "  keras.saving.save_model(model, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-2sAgbEDSoM"
      },
      "outputs": [],
      "source": [
        "# Define file path to save CNN model\n",
        "save_model(CNN_model, 'CNN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-qKGOC5cLqU"
      },
      "outputs": [],
      "source": [
        "# Define file path to save LSTM model\n",
        "save_model(LSTM_model, 'LSTM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcObgaCSDCVz"
      },
      "outputs": [],
      "source": [
        "# Define file path to save SNN model\n",
        "save_model(BiLSTM_model, 'BiLSTM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HozSPPLBcOkn"
      },
      "outputs": [],
      "source": [
        "# Define file path to save CNN + LSTM model\n",
        "save_model(HybridCL_model, 'CNN_LSTM')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file path to save CNN + LSTM model\n",
        "save_model(HybridCB_model, 'CNN_BiLSTM')"
      ],
      "metadata": {
        "id": "ZHsudfU4ku1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Time**"
      ],
      "metadata": {
        "id": "yMEr0yJGZCQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"training_time.txt\", \"w\")\n",
        "for time in training_times.values():\n",
        "  f.write(\"%s\\n\" % time)\n",
        "  print(time)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "OVzn2W18Y-q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#open and read the file after the appending:\n",
        "f = open(\"training_time.txt\", \"r\")\n",
        "print(f.read())"
      ],
      "metadata": {
        "id": "XJvuhZ4QZ-R7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QY2tXOwOyC1a",
        "yPOtM9imlXX4",
        "VzMYQjuQyz46",
        "NEb8qaOdyBb3",
        "3ie9NEKFnh3O",
        "Rw-kdtILR3EE",
        "3p8djyeDC_VO"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}